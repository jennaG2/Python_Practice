{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# import Python packages as needed\n",
    "# NOTE: you can choose to install/use external packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: word association mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_filepath = './amazon_reviews.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of reviews: 30000\n",
      "total number of positive reviews: 15091\n",
      "total number of negative reviews: 14909\n"
     ]
    }
   ],
   "source": [
    "num_reviews = 0\n",
    "num_positive_reviews = 0\n",
    "num_negative_reviews = 0\n",
    "with open(review_filepath) as f:\n",
    "    f.readline() # skip header (the first line) \n",
    "    for line in f:\n",
    "        num_reviews += 1\n",
    "        if line.strip().split()[0] == '1':\n",
    "            num_positive_reviews += 1\n",
    "        else:\n",
    "            num_negative_reviews += 1\n",
    "print ('total number of reviews:', num_reviews)\n",
    "print ('total number of positive reviews:', num_positive_reviews)\n",
    "print ('total number of negative reviews:', num_negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count frequency of single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    for punctuations in [',', '.', '\"', '!', '?', ':', ';', '-', '(', ')', '[', ']']:\n",
    "        text = text.replace(punctuations, ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of single words (aka. unigrams) in the corpus\n",
    "# Parameter:\n",
    "#       filepath: file path of amazon_review.txt\n",
    "# Return: \n",
    "#       a dictionary, key = word, value = word frequency\n",
    "\n",
    "def get_single_word_frequency(filepath):\n",
    "    word_freq = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            review_text = process_text(line.split('\\t')[1])\n",
    "            for word in review_text.split():\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                else:\n",
    "                    word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 119835\n",
      "and 64619\n",
      "i 63045\n",
      "a 60750\n",
      "to 57968\n",
      "it 47813\n",
      "of 47382\n",
      "this 44363\n",
      "is 41185\n",
      "in 27962\n"
     ]
    }
   ],
   "source": [
    "word_freq = get_single_word_frequency(review_filepath)\n",
    "for word, freq in sorted(word_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 69034\n",
      "total number of word occurrences: 2384092\n"
     ]
    }
   ],
   "source": [
    "total_num_words = sum(word_freq.values())\n",
    "print ('number of unique words:', len(word_freq))\n",
    "print ('total number of word occurrences:', total_num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count frequency of ordered pair of words in a text window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of text windows that contain an ordered pair of words\n",
    "# Parameter:\n",
    "#       filepath: file path of amazon_review.txt\n",
    "#       window_size: the size of a text window (measured in number of words)\n",
    "# Return: \n",
    "#       a dictionary, key = ordered word pair (a tuple), \n",
    "#                     value = number of text windows containing this pair\n",
    "\n",
    "def get_ordered_word_pair_frequency(filepath, window_size):\n",
    "    pair_freq = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            review_text = process_text(line.split('\\t')[1])\n",
    "            word_list = review_text.split()\n",
    "            for i in range(len(word_list)):\n",
    "                for j in range(i + 1, len(word_list)):\n",
    "                    # only consider pairs of words no more than window_size apart  \n",
    "                    if j - i + 1 >= window_size:\n",
    "                        break\n",
    "                    # put this ordered word pair into a tuple\n",
    "                    order_word_pair = (word_list[i], word_list[j])\n",
    "                    # accumulate counts\n",
    "                    if order_word_pair not in pair_freq:\n",
    "                        pair_freq[order_word_pair] = 1\n",
    "                    else:\n",
    "                        pair_freq[order_word_pair] += 1\n",
    "    return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of', 'the') 15208\n",
      "('the', 'of') 12728\n",
      "('to', 'the') 11803\n",
      "('this', 'is') 10832\n",
      "('the', 'the') 10615\n",
      "('and', 'the') 10479\n",
      "('in', 'the') 9092\n",
      "('the', 'and') 8728\n",
      "('the', 'is') 8420\n",
      "('is', 'a') 8106\n"
     ]
    }
   ],
   "source": [
    "TEXT_WINDOW_SIZE = 5\n",
    "pair_freq = get_ordered_word_pair_frequency(review_filepath, TEXT_WINDOW_SIZE)\n",
    "for pair, freq in sorted(pair_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(pair, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate pointwise mutual information for each ordered pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_PAIR_FREQUENCY_THRESHOLD = 50\n",
    "pmi_per_pair = {}\n",
    "for pair, freq in pair_freq.items():\n",
    "    if freq < WORD_PAIR_FREQUENCY_THRESHOLD: # filter out infrequent word pairs\n",
    "        continue\n",
    "    if pair[0] in word_freq and pair[1] in word_freq:\n",
    "        # calculate the pointwise mutual information for this pair of words\n",
    "        # you may or may not need to use the following variables:\n",
    "        # \n",
    "        # freq: frequency of this word pair\n",
    "        # word_freq[pair[0]]: frequency of the first word in the pair\n",
    "        # word_freq[pair[1]]: frequency of the second word in the pair\n",
    "        # total_num_words: total number of words in the corpus (i.e. corpus size)\n",
    "        # \n",
    "        # pmi_per_pair[pair] = \n",
    "        \n",
    "        continue # once you fill out the code, you can comment out this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort word pairs in pmi_per_pair by their PMI from highest to lowest. Show the top 100 pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: feature selection using Chi-square statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each word, count how many positive (negative) documents it appears in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of documents that has a specified sentiment and contain a single word  \n",
    "# Parameter:\n",
    "#       filepath: file path of amazon_review.txt\n",
    "#       label: string '0' (negative) or '1' (positive).   \n",
    "# Return: \n",
    "#       a dictionary, key = word, value = word frequency\n",
    "\n",
    "def get_single_word_doc_frequency_per_label(filepath, label):\n",
    "    word_freq_per_label = {}\n",
    "    with open(filepath) as f:\n",
    "        f.readline() # skip header (the first line) \n",
    "        for line in f:\n",
    "            sentiment_label = line.split('\\t')[0].strip()\n",
    "            if sentiment_label == label:\n",
    "                review_text = process_text(line.split('\\t')[1])\n",
    "                for word in set(review_text.split()):\n",
    "                    if word not in word_freq_per_label:\n",
    "                        word_freq_per_label[word] = 1\n",
    "                    else:\n",
    "                        word_freq_per_label[word] += 1\n",
    "    return word_freq_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 13245\n",
      "and 12526\n",
      "a 11891\n",
      "to 11156\n",
      "this 11082\n",
      "i 10334\n",
      "is 10106\n",
      "of 9998\n",
      "it 9793\n",
      "in 8182\n"
     ]
    }
   ],
   "source": [
    "# number of positive documents that contain a word \n",
    "positive_word_freq = get_single_word_doc_frequency_per_label(review_filepath, '1')\n",
    "for word, freq in sorted(positive_word_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 13615\n",
      "and 11743\n",
      "a 11675\n",
      "to 11463\n",
      "i 11415\n",
      "this 11379\n",
      "it 10358\n",
      "of 10128\n",
      "is 9420\n",
      "not 8236\n"
     ]
    }
   ],
   "source": [
    "# number of negative documents that contain a word \n",
    "negative_word_freq = get_single_word_doc_frequency_per_label(review_filepath, '0')\n",
    "for word, freq in sorted(negative_word_freq.items(), key = lambda x: x[1], reverse = True)[:10]:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Chi-square statistic for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# contingency table per word:\n",
    "#                                             sentiment\n",
    "#                       positive                            negative\n",
    "#               ------------------------------------------------------------------------\n",
    "#       present | word present, positive sentiment | word present, negative sentiment |  \n",
    "# word          ------------------------------------------------------------------------\n",
    "#       absent  | word absent,  positive sentiment | word absent, negative sentiment  |  \n",
    "#               ------------------------------------------------------------------------\n",
    "#      \n",
    "\n",
    "chi2_per_word = {}\n",
    "for word, freq in word_freq.items():\n",
    "#     if freq < 10: # filter infrequent words\n",
    "#         continue\n",
    "    if word in positive_word_freq and word in negative_word_freq:        \n",
    "        # calculate the Chi-square statistic for this word\n",
    "        # you may or may not need to use the following variables:\n",
    "        # \n",
    "        # positive_word_freq[word]: number of positive reviews where this word is present\n",
    "        # negative_word_freq[word]: number of negative reviews where this word is present \n",
    "        # num_positive_reviews: number of positive reviews in total \n",
    "        # num_negative_reviews: number of negative reviews in total\n",
    "        # num_reviews: total number of reviews in the corpus\n",
    "        # \n",
    "        # chi2_per_word[word] = \n",
    "        \n",
    "        continue # once you fill out the code, you can comment out this line\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort words in chi2_per_word by their Chi-square value from highest to lowest. Show the top 100 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: spell correction using letter n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list_filepath = './enwiktionary.a.list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "with open(a_list_filepath) as f:\n",
    "    for line in f:\n",
    "        a_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words/phrases in the list: 305868\n"
     ]
    }
   ],
   "source": [
    "print ('number of words/phrases in the list:', len(a_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_word_into_letter_ngrams(word, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(word)-n+1):\n",
    "        ngrams.append( word[i : i+n] )\n",
    "    return set(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hel', 'rld', 'o w', 'orl', 'lo ', 'ell', ' wo', 'wor', 'llo'}\n"
     ]
    }
   ],
   "source": [
    "print (chunk_word_into_letter_ngrams('hello world', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need a function that can calculate the Jaccard similarity for any pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You also need a function that calculates the edit distance for any pair of words\n",
    "# (You can use an external package to calculate edit distance, e.g. the \"editdistance\" package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each given string, you need to find a list of 10 correctly-spelled words from enwiktionary.a.list\n",
    "#     that have the _highest_ n-gram Jaccard similarity to the given word\n",
    "# Different lengths of the n-grams (i.e., different n) will likely produce a different list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each given string, you need to find a list of 10 correctly-spelled words from enwiktionary.a.list\n",
    "#     that have the _lowest_ edit distance to the given word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
